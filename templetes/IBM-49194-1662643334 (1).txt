AI-BASED LOCALIZATION AND 
CLASSIFICATION OF SKIN DISEASE WITH ERYTHEMA 

A PROJECT REPORT
                                          
                                            Submitted by

TEAM ID                  PNT2022TMID

TEAM                       TEAM LEADER            SUGANYA R
                                   TEAM MEMBER 1      ANANTHA SELVI A
                                   TEAM MEMBER 2      HARITHA SHALU S
                                   TEAM MEMBER 3      PRIYA DHARSHINI A
 
in partial fulfillment for the award of 

the degree of

BACHELOR OF ENGINEERING
IN
COMPUTER SCIENCE AND ENGINEERING

STELLA MARYS COLLEGE OF ENGINEERING,
  ARUTHENGANVILAI.

NNA UNIVERSITY: CHENNAI 600 025

                                                            NOVEMBER2022
ACKNOWLEDGEMENT

We thank our GOD ALMIGHTY who is the source of knowledge and who guided in all aspects to bring out this project a successful one.
We express our hearty gratitude and sincere thanks to the chairman 
Dr. NAZARETHCHARLES, Ex. Indian Navy. for his encouragement and rendering a platform for completin the project in a successful manner.
We are extremely grateful to Dr. S. SURESH PREMIL KUMAR, M.E.,Ph.D., Principal of Stella Mary’s College of Engineering for being a source of inspiration throughout our study.
It’s our solace to thank Dr. F.R. SHINY MALAR M.Tech., Ph.D.,Head of Department of Computer Science and Engineering for her continuous encouragement to complete our project.
We extend our sincere thanks to Mrs. S. MAMITHA M.E., our Internal guide for her inspiring guidance and valuable advice throughout the completion of our project.
Words are inadequate in offering our thanks to IBM’s NALAIYA THIRAN, for a great learning experience and giving us a lot of knowledge on cloud and IT forums.









TABLE OF CONTENTS

CHAPTER 
NO 
TITLE   
PAGE NO

ABSTRACT  
LIST OF FIGURES   
LIST OF ABBREVIATIONS                                              

1
INTRODUCTION                                               
1.1 Project Overview
1.2 Purpose
6
2
LITERATURE SURVEY 8
2.1 Existing problem
2.2 References
2.3 Problem Statement                 Definition
9
3
IDEATION AND PROPOSED SOLUTION 
3.1 Empathy Map Canvas
3.2 Ideation and Brainstorming
3.3 Proposed Solution
3.4 Problem Solution fit
12
4
REQUIREMENT ANALYSIS 
4.1 Functional Requirement
4.2 Non- Functional Requirement

5
PROJECT DESIGN 
5.1 Data Flow Diagrams
5.2 Solutions and Technical       Architecture
5.3 User Stories

6
PROJECT PLANNING & SCHEDULING 
6.1 Sprint Planning & Estimation
6.2 Sprint Delivery Schedule
6.3 Reports from JIRA

7
CODING AND SOLUTIONING 
7.1 Feature 1
7.2Feature 2

8
TESTING
8.1 Test Cases
8.2 User Acceptance Testing

9
RESULTS
9.1 Performance Metrics

10
ADVANTAGES & DISADVANTAGES

11
CONCLUSION

12
FUTURE SCOPE

13
APPENDIX
Source Code
GitHub & Project Demo Link

INTRODUCTION
1.1 Project Overview :

Although computer-aided diagnosis (CAD) is used to improve the quality of diagnosis in various medical fields such as mammography and colonography, it is not used in dermatology, where noninvasive screening tests are performed only with the naked eye, and avoidable inaccuracies may exist. This study shows that CAD may also be a viable option in dermatology by presenting a novel method to sequentially combine accurate segmentation and classification models. Given an image of the skin, we decompose the image to normalize and extract high-level features. Using a neural network-based segmentation model to create a segmented map of the image, we then cluster sections of abnormal skin and pass this information to a classification model. We classify each cluster into different common skin diseases using another neural network model. Our segmentation model achieves better performance compared to previous studies, and also achieves a near-perfect sensitivity score in unfavorable conditions. Our classification model is more accurate than a baseline model trained without segmentation, while also being able to classify multiple diseases within a single image. This improved performance may be sufficient to use CAD in the field of dermatology. 
 
 1.2 Purpose :
Computer-aided diagnosis (CAD) is a computer-based system that is used in the medical imaging field to aid healthcare workers in their diagnoses. CAD has become a mainstream tool in several medical fields such as mammography and colonography. However, in dermatology, although skin disease is a common disease, one in which early detection and classification is crucial for the successful treatment and recovery of patients, dermatologists perform most noninvasive screening tests only with the naked eye. This may result in avoidable diagnostic inaccuracies as a result of human error, as the detection of the disease can be easily overlooked. Furthermore, classification of a disease is difficult due to the strong similarities between common skin disease symptoms. Therefore, it would be beneficial to exploit the strengths of CAD using artificial intelligence techniques, in order to improve the accuracy of dermatology diagnosis. This paper shows that CAD may be a viable option in the field of dermatology using state-of-the-art deep learning models. 
The segmentation and classification of skin diseases has been gaining attention in the field of artificial intelligence because of its promising results. Two of the more prominent approaches for skin disease segmentation and classification are clustering algorithms and support vector machines (SVMs). Clustering algorithms generally have the advantage of being flexible, easy to implement, with the ability to generalize features that have a similar statistical variance. Trabelsi et al. experimented with various clustering algorithms, such as fuzzy c-means, improved fuzzy cmeans, and K-means, achieving approximately 83% true positive rates in segmenting a skin disease. Rajab et al.implemented an ISODATA clustering algorithm to find the optimal threshold for the segmentation of skin lesions. An inherent disadvantage of clustering a skin disease is its lack of robustness against noise. Clustering algorithms rely on the identification of a centroid that can generalize a cluster of data. Noisy data, or the presence of outliers, can significantly degrade the performance of these algorithms. Therefore, with noisy datasets, caused by images with different types of lighting, non-clustering algorithms may be preferred; however, Keke et al. implemented an improved version of the fuzzy clustering algorithm using the RGB, HSV, and LAB color spaces to create a model that is more robust to noisy data. SVMs have gained attention for their effectiveness in high-dimensional data and their capability to decipher “…subtle patterns in noisy and complex datasets”. Lu et al. segmented erythema in the skin using the radial basis kernel function that allows SVMs to separate nonlinear hyperplanes. Sumithra et al. combined a linear SVM with a k-NN classifier to segment and classify five different classes of skin lesions. Maglogiannis et al. implemented a threshold on the RGB value for segmentation and used an SVM for classification. Although more robust than clustering algorithms, SVMs are more reliant on the preprocessing of data for feature extraction. Without preprocessing that allows a clear definition of hyperplanes, SVMs may also underperform. 
Owing to the disadvantages of these traditional approaches, convolution neural networks (CNNs) have gained popularity because of their ability to extract high-level features with minimal preprocessing. CNNs can expand the advantages of SVMs, such as robustness in noisy datasets without the need for optimal preprocessing, by capturing image context and extracting high-level features through down-sampling. CNNs can interpret the pixels of an image within its own image-level context, as opposed to viewing each pixel in a dataset-level context. However, although down-sampling allows CNNs to view an image in its own context, it degrades the resolution of the image. Although context is gained, the location of a target is lost through downsampling. This is not a problem for classification, but causes some difficulty for segmentation, as both the context and location of the target are essential for optimal performance. To solve this, up-sampling is needed, which works in a manner opposite to that of down-sampling, in the sense that it increases the resolution of the image. While down-sampling takes a matrix and decreases it to a smaller feature map, up-sampling takes a feature map and increases it to a larger matrix. By learning to accurately create a higher-resolution image, CNNs can determine the location of the targets to segment. Thus, for segmentation, we use a combination of down-sampling and upsampling, whereas for classification, we use only down-sampling. To further leverage the advantages of CNNs, skip-connections were introduced, which provided a solution to the degradation problem that occurs when CNN models become too large and complex. We implement skip-connections in both segmentation and classification models. In the segmentation model, blocks of equal feature numbers are connected between the down and up-sampling sections. In the classification model, these skip-connections exist in the form of inverted residual blocks. This allows our models to grow in complexity without any performance degradation. 
In this paper, we present a method to sequentially combine two separate models to solve a larger problem. In the past, skin disease models have been applied to either segmentation or classification. In this study, we sequentially combine both models by using the output of a segmentation model as input to a classification model. In addition, although past studies of nonCNN segmentation models used innovative preprocessing methods, recent CNN developments have focused more on the architecture of the model than on the preprocessing of data. As such, we apply an innovative preprocessing method to the data of our CNN segmentation model. The methods described above lack the ability to localize and classify multiple diseases within one image; however, we have developed a method to address this problem. Our objective is two-fold. First, we show that CAD can be used in the field of dermatology. Second, we show that state-ofthe-art models can be used with current computing power to solve a wider range of complex problems than previously imagined. We begin by explaining the results of our experimentation, followed by a discussion of our findings, a more detailed description of our methodology, and finally, the conclusions that can be drawn from our study. 
LITERATURE SURVEY
2.1 Existing Problem :
Artifical Neural Network(ANN).:
An artificial neuron network (ANN) is a statistical nonlinear predictive modelling method which is used to learn the complex relationships between input and output. The structure of ANN is inspired by the biological pattern of our brain neuron [2]. An ANN has three types of computation node. ANNs learn computation at each node through back-propagation. There are two sorts of data set trained and untrained data set which produces the accuracy by employing a supervised and unsupervised learning approach with different sort of neural network architectures like feed forward, back propagation method which uses the info set at a special manner. Using Artificial Neural Network, accuracy obtained in various researches is 80% which isnt optimum [2]. Also, ANNs require processors with parallel processing power. ANN produces a probing solution it does not give a clue as to why and how it takes place which reduces trust in the network.
Back Propagation Network(BPN):
.Back propagation, a strategy in Artificial Neural Networks to figure out the error contribution of each neuron after a cluster of information (in image recognition, multiple images) is processed. Back Propagation is quite sensitive to noisy and uproarious data. The BNN classifier achieves 75%-80% accuracy [2]. BNN is benefits on prediction and classification but the processing speed is slower compared to other learning algorithms [5] [2].
Support Vector Machine(SVM).:
SVM is a supervised non-linear classifier which constructs an optimal n-dimensional hyperplane to separate all the data points in two categories [2]. In SVM, choosing an honest kernel function isnt easy. It requires long 
cannot make small calibrations to the model and it becomes difficult to tune the parameters used in SVMs. SVMs when compared with ANNs always give best results [3].
2.2 References:
1. Rajab, M. I., Woolfson, M. S. & Morgan, S. P. Application of region-based segmentation and neural network edge detection to skin lesions. Comput. Med. Imaging Graph.28, 61– 68. (2004).  
2. Keke, S., Peng, Z. & Guohui, L., Study on skin color image segmentation used by fuzzyc-means arithmetic. In 2010 Seventh International Conference on Fuzzy Systems and Knowledge Discovery, Yantai, 612–615. (2010).
3. Hongmao, S. Quantitative Structure-Activity Relationships: Promise, Validations, and Pitfalls in A Practical Guide to Rational Drug Design 163–192 (Woodhead Publishing, Sawston, 2016).  
4. Lu, J., Manton, J. H., Kazmierczak E. & Sinclair, R., Erythema detection in digital skin images. In 2010 IEEE International Conference on Image Processing, Hong Kong, 2545–2548.(2010). 
5. Sumithra, R., Suhil, M. & Guru, D. S. Segmentation and classification of skin lesions for disease diagnosis. Proced. Comput. Sci.45, 76–85. (2015).
6. Maglogiannis, I., Zafiropoulos, E. & Kyranoudis, C. Intelligent segmentation and classification of pigmented skin lesions in dermatological images in Advances in Artificial Intelligence. SETN 2006. In Lecture Notes in Computer Science Vol. 3955 (eds Antoniou, G. et al.) 214–223 (Springer, Berlin, 2006). 
7. Albawi, S., Mohammed, T. A. & Al-Zawi, S., Understanding of a convolutional neural network. In 2017 International Conference on Engineering and Technology (ICET), Antalya, 1–6. (2017).
8. Selvaraju, R. et al. Grad-CAM: Visual explanations from deep networks via gradientbased localization. Int. J. Comput. Vis.128, (2019). 

9. Gutman, D., Codella, N., Celebi, E., Helba, B., Marchettic, M., Mishra, N., & Halpern, A., Skin Lesion Analysis toward Melanoma Detection: A Challenge at the International Symposium on Biomedical Imaging (ISBI) 2016, hosted by the International Skin Imaging Collaboration (ISIC).(2016). 
10. Codella, N., Gutman, D., Celebi, ME., Helba, B., Marchetti, MA., Dusza, S., Kalloo, A., 
Liopyris, K., Mishra, N., Kittler, H., & Halpern, A., Skin Lesion Analysis Toward Melanoma Detection: A Challenge at the 2017 International Symposium on Biomedical Imaging (ISBI), Hosted by the International Skin Imaging Collaboration (ISIC). (2017). 
 
11. Tschandl, P., Rosendahl, C. & Kittler, H. The HAM10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions. Sci. Data5,(2018). 
12. Tsumura, N., Haneishi, H. & Miyake, Y. Independent-component analysis of skin color image. J. Opt. Soc. Am. A16, 2169–2176.(1999). 
13. Hyvärinen, A. & Oja, E. Independent component analysis: Algorithms and applications. Neural Netw.13, 411–430. (2000). 
14. Ronneberger, O., Fischer, P. & Brox, T. U-net: Convolutional networks for biomedical image segmentation. Medical image computing and computer-assisted intervention— MICCAI 2015. MICCAI 2015. In Lecture Notes in Computer Science Vol. 9351 (eds Navab, N. et al.) 234–241 (Springer, Berlin, 2015).
15. Taha, A. & Hanbury, A. An efficient algorithm for calculating the exact hausdorff distance. IEEE Trans. Pattern Anal. Mach. Intell.37(11),(2015). 
16. Tan, M. & Le, Q., Efficientnet: Rethinking model scaling for convolutional neural networks, in ICML, 6105–6114.  (2019). 
2.3 Problem Satement Definition:
Skin diseases include all conditions that irritate, clog or damage your skin, as well as skin cancer. You may inherit a skin condition or develop a skin disease. Many skin diseases cause itchiness, dry skin or rashes. Often, you can manage these symptoms with medication, proper skin care and lifestyle changes
IDEATION & PROPOSED SOLUTION

3.1 Empathy Map Canvas :
An empathy map is a simple, easy-to-digest visual that captures knowledge about a user’s behaviours and attitudes. 
 
It is a useful tool to helps teams better understand their users.
Creating an effective solution requires understanding the true problem and the person who is experiencing it. The exercise of creating the map helps participants consider things from the user’s perspective along with his or her goals and challenges.
Example:
AI-based localization and classification of skin disease with erythema
	

Fig 3.3) Empathy Map Canvas
 3.2 Ideation & Brainstorming :
Ideation is often closely related to the practice of brainstorming, a specific technique that is utilized to generate new ideas. A principal difference between ideation and brainstorming is that ideation is commonly more thought of as being an individual pursuit, while brainstorming is almost always a group activity.


 Fig 3.2.1) Brainstorming 
Fig 3.2.2) Group ideas
Fig 3.2.3) Preparation Chart

3.3 Proposed Solution : 
Project team shall fill the following information in the proposed solution template.
 
S.no
Parameter 
Description 
1. 
ProblemStatement(Problemtobe solved) 
User is suffering from skin disease and needs  immediate assistance and is unsure of how to handle it. 
2. 
Idea/Solution description 
The User  has the ability to take pictures of skin, which are then sent to a trained model.  
 
The model examines the photograph and determines whether or not the subject has a skin disease. 
3. 
Novelty/Uniqueness
Images with noise have also been taken an dareenhanced with effective algorithms forpredictingthediseases. 
Using the camera on their device, users may detect and identify their skin issues. This website can process variation of all sizes and forms. 
4. 
SocialImpact/CustomerSatisfaction
Different skin disorders can be detected by just submitting photographs, and this approach is quite helpful in assisting people in the community identify infections earlier. 
5. 
BusinessModel(RevenueModel)
Our return on assets will be the creation and distribution of a proprietary product that will function as a solution. 
6. 
ScalabilityoftheSolution
This approach is more scalable because it handles any images type, whatever of resolution, and gives great performance in any circumstances. 

Fig3.3) Proposed Solution Chart
3.4 Problem Solution fit:
          These are used to treat skin conditions including eczema and come in many forms including foams, lotions, ointments and creams. Retinoids: These medications (such as retin-A and tazorac) are gels or creams derived from vitamin A and are used to treat conditions including acne
	

Fig 3.4) Structure of solution fit




REQUIREMENT ANALYS

4.1 Functional Requirement :

	These are the requirements that the end user specifically demands as basic facilities that the system should offer. All these functionalities need to be necessarily incorporated into the system as a part of the contract. These are represented or stated in the form of input to be given to the system, the operation performed and the output expected. They are basically the requirements stated by the user which one can see directly in the final product. Following are the functional requirements of the proposed solution. 


FR No.
 Functional Requirement (Epic)
Sub Requirement (Story / Sub-Task)
FR-1 
User Registration 
1. Registration through Form 
2. Registration through Gmail 
3. Registration using phone, laptop, computer 
FR-2 
User Confirmation 
1. Confirmation via Email 
2. Confirmation via OTP 
FR-3 
User Profile 
 Users provides their medical history 
FR-4 
User Interface 
1. User login form 
2. Admin login form 
FR-5 
User Uploads Images (Input) 
1. Upload Images as jpeg 
2. Upload Images as png 
FR-6 
Output Analysis 
 Output analyzed through trained model 
Fig 4.) Functional Requirements
Advantages :
● It allows you to determine if the application has all the functionalities specified in the functional requirements. 
● The most cost-effective time to correct errors is during the functional requirement gathering stage.
4.2 Non-functional Requirements: 
These are basically the quality constraints that the system must satisfy according to the project contract. The priority or extent to which these factors are implemented varies from one project to other. They are also called non-behavioral requirements. Following are the non-functional requirements of the proposed solution. 



FR No.
Non-Functional Requirement
Description
NFR-1 
Usability 
 Used to classify skin disease with erythema 
NFR-2 
Security 
1. It ensures about patient safety during process 
2. It prevents unauthorized individuals from accessing user’s data 
NFR-3 
Reliability 
 Even with more users, there will be a good performance without failure  Less time consumption 
NFR-4 
Performance 
1. With greater accuracy, the performance is high 
2. The trained model can predict an accurate result and took less time when compare to reality 
NFR-5 
Availability 
1. With a good system, all authorized users can access it 
2. Helps to get correct treatment at a correct time, which helps patients to heal earlier 
NFR-6 
Scalability
 Performance will be good even with the higher user traffic 
Fig 4.2) Non-functional Requirements
Advantages :
● They ensure the software system follows legal and adherence rules. 
● They ensure good user experience, ease of operating the software, and minimize the cost factor. 
PROJECT DESIGN

5.1 Data Flow Diagrams:  
A Data Flow Diagram (DFD) is a traditional visual representation of the information flows within a system. A neat and clear DFD can depict the right amount of the system requirement graphically. It shows how data enters and leaves the system, what changes the information, and where data is stored. 
 \
Fig 5.1) Data Flow Diagrams
 
 
 Fig 5.2) Data Flow Diagrams

 5.2 Solution & Technical Architecture

Fig 5.2 ) Solution & Technical Architecture

Fig 5.2.1) Technical Architecture

5.3 User Stories  
Use the below template to list all the user stories for the product. 
User Type   
Functional  
Requirement 
(Epic)  
U ser Story 
Number  
 
User Story / Task  
 
Acceptance criteria  
 
Priority  
 
Release  
 







Customer 
(Mobile user)  
 
Registration  
 
USN-1  
 
As a user, I can register for the application by entering my email, password, and confirming my password.  
 
I can access my account / dashboard  
 
High  
 
Sprint-1  
 
 
 
USN-2  
 
As a user, I will receive 
confirmation email once I have registered for the application  
I can receive confirmation email & 
click confirm  
 
High  
 
Sprint-1
 
 
USN-3  
 
As a user, I can register for the application through Facebook  
I can register & access the dashboard with Facebook Login  
 
Low  
 
Sprint-2  
 
 
 
USN-4  
 
As a user, I can register for the application through Gmail  
 
 
Medium  
 
Sprint-1
 
Login  
 
USN-5  
 
As a user, I can log into the application by entering email & password  
 
 
High  
 
Sprint-1  
 
 
Dashboard  
USN-5  
 
As a user, I can 
Access my 
Dashboard  
 
 
Medium  
 
Sprint-3  
 
Customer (Web user)  
Registration  
 
USN-1  
 
As a user, I can register for the application by entering my email, password, and confirming  my password
I can access my account / dashboard  
 
High  
 
Sprint-4  
 
Customer 
Care  
Executive 
Solution  
 
USN-5  
 
Responding to each email you receive can make a lasting impression on customers 
Offer a solution for 
how your company can improve the customer’s experience  
 
High  
Sprint-3  
 
Administrator  
 
Manage  
 
USN-5  
 
Do-it-yourself service for delivering Everything  
 
setof predefined requirements that must be  met to mark a user story complete.  
High  
 
Sprint-4  
 

Fig 5.3) User Stories





PROJECT PLANNING & SCHEDULING

6.1 Sprint Planning &Estimation

S.NO
ACTIVITY  TITLE
ACTIVITY DESCRIPTION
DURATION

Understanding the project requirement
Assign the team members and create repository in the Github,Assign the task to each members and teachhow to use and open andclass the Github
and IBM career education



      1 WEEK
2
Starting of project
Advice students to attend classes of IBM portalcreate and
develop an rough diagrambased on project decription and gather of information on IOT and IBM project and teamleader assigntask to each member of the project





2 WEEK
3
Attend class
Team members and team 
must watch and learnfrom classes provided by IBM and
NALAYATHIRAN and must gain access of MIT license for their project





3 WEEK
4
Budget           and scopeof project
Budget and analyze the use of IOT in the projectand discuss with team for budgetprediction to predict the favourability for the customerto buy



 4 WEEK

Fig 6.1) Sprint Planning &Estimation
6.2 Sprint Delivery Schedule

Sprint
Functional   Requirement (Epic )
User Story Number
User Story/Task
Story Points
Priority
Team Members
Sprint-1
Create Dataset and Annotate
Images
USN-1
Create the dataset with 50 images per skin disease .Annotate images using Microsoft VOTT .
20
High
Haritha shalu.S, 
Suganya.R
Ananthaselvi.A,Priyadharshini.A
Sprint-2
T raining YO LO
USN-2
Download and convert pre-trained weights. Train Yolov3 detector and build the source 
20
High
R.Suganya
Haritha shalu.S
Sprint-3
Cloudant DB
USN-3
Create cloud account, create
Service instance ,launch cloudant DB and create the database.
20
Low
Anantha selvi.A
Priyadharshini.A
Sprint-4
Login
USN-4
As a user, I can login into the application.
20
Medium
Suganya.R
Haritha shalu.S
Anantha selvi.A
Priyadharshini.A
Sprint-4
 
USN-5
As a user I can upload skin images as input
20
High
Suganya.R
Haritha shalu.S
Anantha selvi.A
Priyadharshini.A
Sprint-4
 
USN-6
As an admin I can track the skin condition of the user and recommend medication for the user and as a user I can logout successfully.
20
Medium
Suganya.R
Haritha shalu.S
Anantha selvi.A
Priyadharshini.A



Sprint
Total Story Points
Duration
Sprint Start Date
Sprint End Date (Planned)
Story Points
Completed (as on Planned
End Date)
Sprint Release
Date (Actual)
Sprint-1
20
6 Days
24 Oct 2022
29 Oct 2022
20
29 Oct 2022
Sprint-2
20
6 Days
31 Oct 2022
05 Nov 2022
20
05 Nov 2022
Sprint-3
20
6 Days
07 Nov 2022
12 Nov 2022
20
12 Nov 2022
Sprint-4
20
6 Days
14 Nov 2022
19 Nov 2022
20
19 Nov 2022
 

Sprint
Total Story  Points
Duration
Sprint Start Date
Sprint End Date (Planned)
Story Points
Completed (as on Planned
End Date)
Sprint Release
Date (Actual)
Sprint-1
20
6 Days
24 Oct 2022
29 Oct 2022
20
29 Oct 2022
Sprint-2
20
6 Days
31 Oct 2022
05 Nov 2022
20
05 Nov 2022
Sprint-3
20
6 Days
07 Nov 2022
12 Nov 2022
20
12 Nov 2022
Sprint-4
20
6 Days
14 Nov 2022
19 Nov 2022
20
19 Nov 2022

6.3 Report from JIRA: 
Sprint-1
Step 1:




Step 2:

Step 3:


Step 4:

Step 5:


Sprint-2
Step 1:

Step 2:

Step 3:

Step 4:

Step 5:

Step 6:

 

Sprint -3
Step 1:



Step 2:

Step 3:


Step 4:


Step 5:



Step 6:

     Sprint-4
Step 1:

 
Step 2:

 
Step 3:

 



Step 4:

 
Step 5:

 


Step 6:



7.CODEING & SOLUTIONING

7.1 Feature 1
Building HTML Pages
Index.html
<!DOCTYPE html>
<html>
    <head>
        <title>skin diseases</title>
        <link rel="stylesheet" href="styles.css">
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<style>
    body {
        font-size: 20px;
        padding: 0%;
        margin: 0%;
    }
    #nav{
        background: #181719;
        text-decoration: none;
    }
    
     li {
        display: inline;
       
    }
    #nav ul{
                text-decoration: none;
                background-color: black;
                color: #00ffff;
                max-height: 5rem;
                min-height: 2rem;
                padding-top: 5px;
                font-size: 20px;
            }
        
    
    #nav a{
        float: right;
        color:#fff;
        text-decoration: none;
        font-size :18px;
        padding-right: 15px;
    }
    h3, header {
        color: rgb(231, 64, 64);
        font-size: 30px;
        padding-left: 20px;
        
        
    }
    h3{
        font-weight: 100%;
        margin-left: 150px;
         font-size: 50px;
    }
    header {
        height: 8px;
        margin-left: 100px;
        font-size: 50px;
    }
    
    #showcase {
        background-image: url(' https://www.girvenfp.co.nz/wp-content/uploads/2017/10/skin-disease.png');
        height: 500px;
        padding-top: 0em;
        background-position:  right;
        
    }
    h4{
        font-size:30px ;
        text-align: center;
    }
    hr {
        background-color: yellow;
        height: 1px;
        width: 20%;
    }
    #right h5{
        font-size: 30px;
        height: 1px;
        margin-left: 10px;
        
    }
    #left h5{
        font-size: 30px;
        height:1px;
        margin-left: 10px;
    }
    
</style>
    </head>
    <body>
        <div class="container">
            <nav id="nav">
            <ul> Skinnovation
            <li><a href="login.html">Prediction</a></li>
            <li><a href="regiser.html">Register</a></li>
            <li><a href="login.html">Login</a></li>
            <li><a href="index1.html">Home</a></li>
            </ul>
            </nav>
            </div>
            <section id="showcase">
            <div class="container">
                <header>Different types of </header>
                <h3>Skin Disorders</h3>
                
            </div></section>
 
            <section>
                    <h4>ABOUT PROJECT<hr></h4>
                <article id="left">
                    <h5>Problem</h5>
                    <p>Skin diseases include all conditions that irritate, clog or damage your skin, as well as skin cancer. You may inherit a skin condition or develop a skin disease. Many skin diseases cause itchiness, dry skin or rashes. Often, you can manage these symptoms with medication, proper skin care and lifestyle changes.Skin disorders vary greatly in symptoms and severity. They can be temporary or permanent, and may be painless or painful. Some have situational causes, while others may be genetic. Some skin conditions are minor, and others can be life-threatening.</p>
                </article>
                <article id="right">
                    <h5>Solution</h5>
                    <p>Skin diseases are conditions that affect your skin. These diseases may cause rashes, inflammation, itchiness or other skin changes. Some skin conditions may be genetic, while lifestyle factors may cause others. Skin disease treatment may include medications, creams or ointments, or lifestyle changes.</p>
                </article>
                <article>
                    <h4>WE CLASSIFY<hr></h4>
                    <center>
                    <img src="https://d3i71xaburhd42.cloudfront.net/975c1cb47e2bd3d54ad1d4f606dda86d0390d3fa/2-Figure1-1.png" alt="classify" width="" height="350px" id="img">
 
                </center>
            <p>A skin disease detection and classification system is a system used for detecting whether a disease is present or not, and then classifying the type of disease, if present. The classification is based on decisions taken using the features extracted through the feature extraction methods.</p></article>
            </section>
        
    </body>
</html>
 





Register.html
<!DOCTYPE html>
<html>
    <head>
        <style>
            #nav ul{
                text-decoration: none;
                background-color: black;
                color: #00ffff;
                max-height: 5rem;
                min-height: 2rem;
                padding-top: 5px;
                font-size: 20px;
            }
            li{
                float: right;
                display: inline;
 
            }
#nav a{
        color: #fff;
        text-decoration: none;
        font-size :18px;
        padding-right: 15px;
            }
.registerbtn {  
    
  background-color: #4CAF50;  
  color: white;  
  margin-top: 10px; 
  border: none;  
  cursor: pointer;  
  margin-left: 100px;
  height: 50px;
  width: 100px;  
  opacity: 0.9;  
  
}  
.registerbtn a{
    font-size: 18px;
    color: black;
    text-decoration: none;
}
.registerbtn:hover {  
  opacity: 1;  
}
body{
    margin: 0;
    padding: 0;
}
.container{
    margin-top: 5px;
    top: 50%;
    left: 50%;
    position: absolute;
    transform: translate(-50%,-50%);
}
.card{
    margin-top: 5px;
    height: 15em;
    background:rgb(255, 187, 99);
    padding: 60px 40px 50px 40px;
    border-radius: 10px;
}
#name {
    margin-top: 10px;
    border: black;
    margin-left: 50px;
    margin-right: 50px;
    margin-bottom: 10px;
    padding: 10px;
}
#acc {
    font-size: 18px;
    margin-top: 20px;
    margin-left: 50px;
}
#img{
    display: block;
  margin-left: auto;
  margin-right: auto;
  width: 50%;
}
#button {
    font-size: 18px;
    color: black;
    text-decoration: none;
}
        </style>
    </head>
    <body>
        <div>
            <nav id="nav">
            <ul>Skinnovation
            <li><a href="regiser.html">Register</a></li>
            <li><a href="login.html">Login</a></li>
            <li><a href="index1.html">Home</a></li>
            </ul>
            </nav>
            </div>
            <div class="container">
                <img src="https://img.icons8.com/plasticine/400/edit-user-male.png" height="200px" id="img">
                <div class="card">
                    <input type="text" name="fistname" placeholder="Enter Your Name" required id="name"><br>
                    <input type="email" name="email" placeholder="Enter Email ID" required id="name"><br>
                    <input type="text" name="password" placeholder="Enter Password" required id="name"><br>
                    <button type="submit" class="registerbtn" id="button"><a href="prediction.html"><b>Register</b></a></button><br> 
                    <div id="acc">
                    Already have an account? <a href="login.html">Login</a>
                </div></div>
            </div>
            </body>
</html>
Login .html
<!DOCTYPE html>
<html>
    <head>
        <style>
            body{
    margin: 0;
    padding: 0;
}
            #nav ul{
                text-decoration: none;
                background-color: black;
                color: #00ffff;
                max-height: 5rem;
                min-height: 2rem;
                padding-top: 5px;
                font-size: 20px;
            }
            li{
                float: right;
                display: inline;
 
            }
#nav a{
        color: #fff;
        text-decoration: none;
        font-size :18px;
        padding-right: 15px;
            }
.container{
    margin-top: 5px;
    top: 50%;
    left: 50%;
    position: absolute;
    transform: translate(-50%,-50%);
}           
.loginbtn {  
    
    background-color: #4CAF50;  
    color: white;  
    margin-top: 50px; 
    margin-top: 50px;
    border: none;  
    cursor: pointer;  
    margin-left: 100px;
    height: 45px;
    width: 100px;  
    opacity: 0.9;  
  }   
  .loginbtn:hover {  
  opacity: 1;  
}
#img{
    display: block;
  margin-left: auto;
  margin-right: auto;
  width: 50%;
}
.card{
    margin-top: 5px;
    height: 15em;
    background:rgb(255, 187, 99);
    padding: 60px 40px 50px 40px;
    border-radius: 10px;
    margin-bottom: 5px;
}
#name {
    margin-top: 10px;
    border: black;
    margin-left: 50px;
    margin-right: 50px;
    margin-bottom: 10px;
    padding: 10px;
    width: max-content;
}
#button a{
    font-size: 18px;
    color: black;
    text-decoration: none;
}
        </style>
    </head>
    <body>
        <div>
            <nav id="nav">
            <ul> Skinnovation
            <li><a href="regiser.html">Register</a></li>
            <li><a href="login.html">Login</a></li>
            <li><a href="index1.html">Home</a></li>
            </ul>
            </nav>
            </div>
            <div class="container">
                <img src="https://img.icons8.com/plasticine/400/edit-user-male.png" height="200px" id="img">
                <div class="card">
                    <input type="email" name="email" placeholder="Enter Email ID"  id="name" required><br>
                    <input type="text" name="password" placeholder="Enter Password"  id="name" required><br>
                    <button type="submit" class="loginbtn" id="button"><a href="prediction.html"><b>Login</b></a></button><br> 
                    </div>
                </body>
</html>
 
7.2 Feature 2
Prediction page.html
<!DOCTYPE html>
<html>
    <head>
        <style>
            body{
    margin: 0;
    padding: 0;
}
#nav{
        background: #181719;
        text-decoration: none;
    }
    
     li {
        display: inline;
       
    }
    #nav ul{
                text-decoration: none;
                background-color: black;
                color: #00ffff;
                max-height: 5rem;
                min-height: 2rem;
                padding-top: 5px;
                font-size: 20px;
            }
        
    
    #nav a{
        float: right;
        color:#fff;
        text-decoration: none;
        font-size :18px;
        padding-right: 15px;
    }
    h4{
        text-align: center;
        font-weight: 150px;
        font-size: 30px;
    }
    hr{
        background-color: yellow;
        padding: 1px;
        width: 65%;
    }
    #img{
        float: right;
        box-sizing: border-box;
        width: 25%;
        padding: 30px;
        margin-right: 10em;
        margin-top: 0%;
    }
    #pr{
        margin-top: 50px;
        margin-left: 50px;
        float: left;
        padding: 0 30px;
        width: 50%;
        box-sizing: border-box;
        box-shadow: #181719;
    }
        </style>
    </head>
    <body>
 
        <div>
            <nav id="nav">
            <ul> Skin Diseases Detection
            <li><a href="logout.html">Logout</a></li>
            <li><a href="index1.html">Home</a></li>
            </ul>
            </nav>
            <h4 class="h">
                Skinnovation-AI-Based localization and classification of skin diseases with erythema<hr>
            </h4>
            <article id="pr">
            <p>Nowadays people are sufferring from skin diseases,More than 125 million people suffering from Psoriasis also skin cancer rate is rapidly increasing over the last few decades especially Melanoma is most diversifying skin cancer.If skin diseases are not treated at an earlier state,then it may lead to complications in the body including spreading of the infection from one individual to the other.This skin disease can be prevented by investicating the infected region atb an early stage.The characterstic of the skin images is diversified so that it is a challenging job to device an efficient and robust algoritham for automatic detection of skin diseases and its scverity.Skin tone and skin colour play an important role in skin diseases detection.Color and coarseness of skin are visually different.Automatic processing of such images for skin analysis requires quantitative discriminator to differentiate the disease.</p>
    </article>
    <img src="C:\Users\ELCOT\Desktop\skin project\images/skin.jpg" max-height="100px" id="img">
        </body>
</html>
 
Logout.html
<!DOCTYPE html>
<html>
    <head>
        <style>
            body{
    margin: 0;
    padding: 0;
    
}
#nav{
        background: #181719;
        text-decoration: none;
        
    }
    
     li {
        display: inline;
       
    }
    #nav ul{
                text-decoration: none;
                background-color: black;
                color: #00ffff;
                max-height: 5rem;
                min-height: 2rem;
                padding-top: 5px;
                font-size: 20px;
            }
        
    
    #nav a{
        float: right;
        color:#fff;
        text-decoration: none;
        font-size :18px;
        padding-right: 15px;
    }
    .logoutbtn {  
    
    background-color: #4CAF50;  
    color: black;  
    cursor: pointer;  
    height: 45px;
    width: 100px;  
    opacity: 0.9;  
  }   
  #button a{
    text-decoration: none;
    color: black;
  }
  .loginbtn:hover {  
  opacity: 1;  
}
.container{
    display: block;
    text-align: center;
    letter-spacing: 1px;
}
p{
    font-size: 20px;
    color: #4CAF50;
    
}
h3{
    font-size: 25px;
}
#img{
    padding: 5px;
    background-color: white;
}
        </style>
    </head>
    <body>
<div>
            <nav id="nav">
            <ul> Skinnovation
            <li><a href="regiser.html">Register</a></li>
            <li><a href="login.html">Login</a></li>
            <li><a href="index1.html">Home</a></li>
            </ul>
            </nav>
            </div>
            <div class="container">
                <img src="C:\Users\ELCOT\Desktop\skin project\images/logout.png" id="img">
                <h3>Successfully Logged Out!</h3>
                <p>Login for more information</p>
                <button type="submit" class="logoutbtn" id="button"><a href="login.html"><b>Login</b></a></button>
            </div>
    </body>
</html>












8.TESTING
8.1 Test Cases:

Step1


Step2

Step3



Step4

 
Step5

8.2 User Accptance Matrics:
Our 2-phase analysis model for localization and classification is shown via the pseudocode in Algorithm 1 and visually in Fig. Fig.5.5. We decomposed the original image into its hemoglobin and melanin constituents using preprocessing, to help our model extract valuable information from data that would have been otherwise unavailable. We provide these images as input to our segmentation model, the U-Net, which generated a segmented image. This segmented image was then analyzed for clusters, which were subsequently cropped and input to our classification model, the EfficientNet, which then produced a classified label, thus completing our analysis model.

Figure 5
Two-phase analysis model. The original image primarily enters a preprocessing stage, where normalization and decomposition occur. Afterwards, the first step is segmentation, where cluster of abnormal skin are segmented and cropped. The second step is classification, where each cluster is classified into its corresponding class.
The data for training and testing were obtained from Dermnet NZ, an archive of skin disease information launched and maintained by a group of dermatologists from New Zealand. The site provides open source images with labels. We selected 18 top-level categories (Table (Table5)5) each of which included enough data, besides including erythema as one of its common symptoms. Using a web crawler, we gathered a total of 15,851 images. Among the images obtained through Dermnet, the erythema of 100 images was masked by dermatologists, to be used as a ground truth. For segmentation, 60 images were used for training, and 40 images were used for testing. For classification, 13,473 images were used for training, and 2,378 images were used for testing. In addition, the test set for classification was split before segmentation cropping to prevent the subsections of one image from appearing in both the training and testing sets. Table Table66 shows the distribution of data in greater detail. We chose the 100 images for segmentation in a balanced manner from each class, to minimize any bias that could occur during the classification phase.
Table 5
Categories for classification.
Top-level categories
1. Acne and Rosacea
2. Actinic keratosis
3. Atopic dermatitis
4. Bullous disease
5. Cellulitis
6. Contact dermatitis
7. Eczema
8. Exanthems
9. Fungal infections
10. Herpes
11. Light chain disease
12. Lupus erythematosus
13. Psoriasis
14. Scabies
15. Systemic disease
16. Urticaria
17. Vasculitis
18. Viral infections


Table 6
Distribution of data in dermnet dataset.
Dataset: Dermnet
Number of data
Segmentation
Classification
Class
Train
Test
Total
Train
Test
Total
Acne and Rosacea
4
2
6
746
131
877
Actinic keratosis
4
2
6
1193
181
1374
Atopic dermatitis
3
2
5
642
120
762
Bullous disease
3
2
5
393
92
485
Cellulitis
3
2
5
223
73
296
Contact dermatitis
3
2
5
231
74
305
Eczema
4
3
7
1667
234
1901
Exanthems
3
2
5
354
87
441
Fungal infections
4
3
7
1601
227
1828
Herpes
3
2
5
397
94
491
Light chain disease
3
2
5
538
117
655
Lupus erythematosus
3
2
5
371
90
461
Psoriasis
4
3
7
2044
275
2319
Scabies
3
2
5
448
98
546
Systemic disease
3
2
5
633
119
752
Urticaria
3
2
5
138
63
201
Vasculitis
3
2
5
411
94
505
Viral infections
4
3
7
1443
209
1652
Total
60
40
100
13,473
2378
15,851

One of the significant merits of the Dermnet dataset is that it was created and is maintained by a diverse group of dermatologists. The images in each top-level category are independent as they are images of different patients at distinct locations taken with varying devices. This is evident in the diverse resolutions, lighting, and aspect ratios of the images. Regardless, it would be optimal to possess a similar dataset from an entirely separate association to truly validate the performance of our model. However, as there are strict regulations regarding the use of data in our private institutions, we utilize publicly available datasets. These datasets were chosen based on the availability of both a segmentation map and a classification label.
ISIC201612, ISIC201713, and HAM1000014 are datasets that have been used in previous AI competitions. They were provided as challenges for both segmentation and classification, and they therefore possess segmentation maps and classification labels. Table Table77 shows a detailed distribution of these datasets. As the ISIC2016 and ISIC2017 datasets also provided a separate test dataset, these datasets were preserved and used for testing. For the HAM10000 dataset, we stratified the dataset according to the classification label, and created a balanced 50% split between the train and test data. There is no separate segmentation dataset, as each image contained a segmentation map. Therefore, all images are used in the training and testing for both segmentation and classification.
Table 7
Distribution of data in dermatoscopic datasets.
Class
Number of data
Train
Test
Total
Dataset: ISIC 2016
Benign
727
303
1030
Malignant
173
75
248
Total
900
378
1278
Dataset: ISIC 2017
Benign
1372
393
1843
Melanoma
374
117
386
Seborrheic keratosis
254
90
521
Total
2000
600
2750
Dataset: HAM 10000
Actinic keratosis
164
163
327
Basal cell carcinoma
257
257
514
Benign
549
550
1099
Dermatofibroma
58
57
115
Melanoma
556
557
1113
Melanocytic nevi
3352
3353
6705
Vascular lesion
71
71
142
Total
5007
5008
10,015

There is one significant difference between these datasets and our Dermnet dataset. The images in these datasets were obtained with a special dermatoscopic device. These devices create high-resolution images with the skin disease located near the center. Therefore, these devices create images similar to the Dermnet dataset images after our segmentation phase. Thus, it is doubtful that our method will demonstrate an improved performance with the dermatoscopic images.
For all datasets, the testing dataset is unused for validation until the end of training. This is done to verify that our models learn to generalize unseen images. We take a three-fold cross-validation approach with training data for validation during training. We generate three replicas of each dataset and create a unique 90-to-10 training and validation set. With each replica, we use a grid search algorithm to test different combinations of hyperparameters. Lastly, we train our model using the entire training set and select our hyperparameters based on the cross-validation stage. Training and testing were performed on a single GTX Titan V and four Intel Xeon Gold 5115 processors. We now explain each section of our analysis model in more detail.
Algorithm 1 AnalyzeSkin
1: procedure SEGMENT(x)
2: h, m = DECOMPOSE(x)
3: mask = U-NET([x, h, m])
4: CLASSIFY(mask)
5: end procedure
6: procedure CLASSIFY(mask)
7: clusters = FINDCLUSTERS(mask)
8: for cluster in clusters do
9: cluster = FIXRATIO(cluster)
10: cluster = RESIZE(cluster)
11: class = EFFICIENTNET(cluster)
12. top_prediction = GETHIGHESTCONFIDENCE(class)
12: print(top_prediction)
13: end for
14: end procedure

Preprocessing: decomposition
The main constituents of the skin that are visible to humans are melanin and hemoglobin. These constituents provide valuable information for the segmentation of abnormal skin. To ensure that our model can learn to use these features, we used independent component analysis (ICA) to extract the melanin and hemoglobin constituents7,15,16. Assuming that these components are linearly separable, the separated linear vectors can be represented by the following formula7:
Lx,y=dmqmx,y+dhqhx,y+Δ
where dm and dh represent the density vectors of melanin and hemoglobin, respectively, qmx,y and qhx,y represent the quantity of these components, and Δ represents values that are caused by other colors. As shown in7, by applying ICA, we can decompose skin as
[qmx,y,,,qhx,y]=D−−1L(x,,,y)−E
E=minx,y(D−−1,L(x,,,y))
Ix,y=exp(−L′x,y)
where D− represents the estimated values of dm and dh, and Ix,y represents the decomposed result. Figure 6 shows an example of one of these decompositions.

Figure 6
Decomposed result of skin. The original image is decomposed into its hemoglobin and melanin constituents through ICA.

Segmentation
The U-Net17, as shown in Fig. Fig.7,7, is an architecture created by CNNs, that has attracted attention for accurate biomedical image segmentation through the combination of down-sampling, up-sampling, and skip connections. Its name is attributed to the shape of its architecture, the first half of the ‘U’ representing down-sampling. Here, the context and key features of the input images are gained at the cost of a decrease in resolution. The second half of the ‘U’ represents up-sampling. Here, the resolution is increased to gain knowledge of the location of the target segment. To combat degradation due to the complexity of the model, skip connections are added to each up-sampling block.

Figure 7
U-Net architecture. A fully CNN network, comprised of down-sampling, up-sampling, and skip connections17.
Although in the original paper17, the resolutions of input and output were different, that is, 572 × 572 and 388 × 388 pixels, respectively, we chose to keep our input and output resolution consistent at 304 × 304 pixels. This was done because the images in our dataset were not large enough to warrant the tiling strategy required for extremely large images. Thus, zero-padding allowed us to keep the input and output resolutions consistent, thereby allowing the retention of information present on the border of our images.
Using the decomposed images, in one instance, we input three images, namely, the original, the hemoglobin, and the melanin images, to our U-Net and obtained a single black-and-white mask image as output as shown in Fig. Fig.8.8. In this image, a black pixel represented normal skin, and a white pixel represented abnormal skin. Using the mask image, we used a simple contour-finding algorithm to draw an outline around clusters of erythema. We then used a convex hull algorithm to draw rectangles around the contours. The dimensions and locations of these rectangles were then used to crop the original image. These cropped images of each cluster were saved as individual pictures. We added padding to each cluster to create a larger and squarer image, as the performance of classification can suffer due to clusters being too small or not evenly shaped. Figure Figure99 shows contours and rectangles around each cluster showing how each cluster was cropped.

Figure 8
Input and Output of the U-Net. The inputs of the U-Net are the original, hemoglobin, and melanin images obtained from the preprocessing step. The output of the U-Net is a single masked image.

Figure 9
Contour finding algorithm applied to output of U-Net. Clusters of abnormal skin are identified through a contour finding algorithm. Each cluster is cropped in the shape of a rectangle through a convex hull algorithm used to surround each contour.
After generating three replicas of our dataset, we create a unique 90-to-10 training and validation set. With each replica, we perform a grid search algorithm to find the optimal hyperparameters. For the loss function, we test the Binary Cross-Entropy and Dice Coefficient Loss. For the optimizer, we test Adam with learning rates of 1e−4, 5e−5, and 1e−5; RMSprop with learning rates of 1e−4, 5e−5, and 1e−5; and SGD with a momentum of 0.9 and learning rates of 1e−1, 5e−2, and 5e−2. For the number of epochs, we test with 40, 60, and 80 epochs and decrease the learning rate by a factor of 0.1 every 20 epochs. After testing with the replicas, we use the full training set for training with the hyperparameters: Binary Cross-Entropy, Adam with a learning rate of 5e−4, a weight decay of 5e−4, 60 epochs, and a decrease in learning rate by a factor of 0.1 every 20 epochs.
As our main objective was to demonstrate the viability of CAD, the performance was mostly determined using pixel-level sensitivity rather than the Intersection over Union or the Dice coefficient metrics that are often used to measure segmentation performance. Moreover, we mainly focused on the true positive rates of segmentation, represented by the sensitivity metric. This is because our aim was to create a screening test method to help healthcare workers make a more accurate diagnosis by preventing abnormal skin from being overlooked. Nevertheless, we also measured the performance of our model using the specificity, Dice coefficient, and Hausdorff distance to provide a more complete performance comparison. We measured these metrics by comparing the output from our U-Net model to an image that was masked by professional dermatologists. Going through each pixel, if a pixel of the U-Net output was black and the pixel of the dermatologist-masked image at the same location was black, this is seen as a true negative. If both were white, this was seen as a true positive. If the U-Net output was black but the dermatologist mask was white, this was seen as a false negative, and the converse was a false positive. The equations for sensitivity, specificity, and Dice coefficient metric can be represented by the following formulas:
Sensitivity=TPTP+FN
Specificity=TNTN+FP
DiceCoef.=2×TP(T,P,+,F,P)+(TP+FN)
The Hausdorff distance (HD) is used to measure the dissimilarity between the predicted segmentation masks the and ground truth. The Hausdorff distance can be calculated by the formula18:
SetX={x1,,,⋯,xn}andY={y1,⋯,yn}
H(X,,,Y)=max(h,(X,,,Y),,,h,(Y,,,X)),
where h(X,,,Y)=maxx∈Xminy∈Y∥x−y∥.
We use an implementation of the method presented18 to calculate the Hausdorff distance between the output and ground truth.
Classification
EfficientNets18 were introduced in late 2019 as a state-of-the-art model for image classification. Rather than scaling a CNN model without balance between the depth, width, and resolution of the image at hand, EfficientNets were developed by scaling a baseline model in a methodical manner. This allows for an efficient increase in accuracy rates without unreasonable amounts of required memory and floating-point operations (FLOPS) through the optimization of the following formulas18:
maxd,w,rAccuracy(N(d,w,r))
suchthat:N(d,,,w,,,r)=⊙i=1⋯sFˆd∙Liˆi(X<r∙Hˆi,r∙Wˆi,w∙Cˆi>)
Memory(N)≤targetmemory
FLOPS(N)≤targetflops
Here, d, w, and r represent the depth, width, and resolution of the scaled model, and Hˆ,Wˆ,Cˆ,Fˆ,Lˆ represent the parameters of the optimized baseline model. Thus, in summary, the goal of the EfficientNet model, namely, N(d,w,r), is to produce maximum accuracy in a classification problem. The model is represented by the product of its variable-weighted parameters, represented as ⊙i=1⋯sFˆd∙Liˆi(X<r∙Hˆi,r∙Wˆi,w∙Cˆi>). The memory usage, Memory(N), and required computational performance, FLOPS(N), for the model must be less than that of the target.
The original paper19 presents eight different models, ranging from EfficientNet-B0 through EfficientNet-B7, each increasing in complexity. Table Table88 shows the accuracy and training time per epoch of each of these models trained on unsegmented images. There are sharp increases in training time between the EfficientNet-B4 and EfficientNet-B7 models, as we were forced to use smaller batch sizes during training owing to the increased number of trainable parameters and the limited memory in our GPU. In addition, as we employ a grid search algorithm, many models must be trained for many epochs. Therefore, a lower training time is desirable. After testing these models with our dataset and hardware, we chose to implement the EfficientNet-B4 model as it used substantial memory and training time without losing excessive complexity. We applied transfer learning to the segmented and cropped images from the previous section and classified them into 18 different classes.
Table 8
Training time required for efficientnet-B0 through B7.
Model
Top-1 accuracy (%)
Training time per epoch (s)
EfficientNet-B0
39.71
187.965
EfficientNet-B1
43.15
250.170
EfficientNet-B2
44.46
255.180
EfficientNet-B3
43.30
309.375
EfficientNet-B4
45.77
392.925
EfficientNet-B5
45.54
522.975
EfficientNet-B6
45.83
643.965
EfficientNet-B7
47.54
942.720

We further improved the performance by using the Synthetic Minority Oversampling Technique20 library, as a more balanced dataset was needed for training. In addition, because our segmentation model required more data to better generalize erythema, there were clusters of normal skin that were cropped and included in different classes. It was observed that this confused the model, as similar images were seen throughout different classes. To combat this, we refined the data by going through each image and excluding certain images that were either too small or incorrectly segmented images.
We created replicas of the training set and performed a grid search algorithm, as in the method utilized in the segmentation phase. For the loss function, we tested the Categorical Cross-Entropy and Focal Loss. For the optimizer, we test Adam with learning rates of 1e−4, 5e−5, and 1e−5; RMSprop with learning rates of 1e−4, 5e−5, and 1e−5; and SGD with a momentum of 0.9 and learning rates of 1e−1, 5e−2, and 5e−2. For the number of epochs, we test with 40 epochs, 60 epochs, and 80 epochs and decrease the learning rate by a factor of 0.1 every 20 epochs. After testing with the replicas, we used the full training set for training with the hyperparameters: Categorical Cross-Entropy, Adam with a learning rate of 1e−5, a weight decay of 5e−4, 80 epochs, and a decrease in learning rate by a factor of 0.1 every 20 epochs. The AUC is calculated by taking the integral of the curve created by points at different sensitivity and specificity thresholds. In addition, specificity, sensitivity, and the F1-score can be represented by the following formulas:
Specificity=TNTN+FP
Sensitivity=TPTP+FN
F1−score=2TP2TP+FP+FN
For all performance metrics, scores are calculated individually for each class present in the dataset. The scores are then weighted and averaged according to the number of data points in a class corresponding to the entire dataset.
Ethics declarations
This study was exempted from the approval by the Institutional Review Board of Seoul National University Boramae Medical Center (No. 07-2020-148). The informed consent was waived by the Institutional Review Board of Seoul National University Boramae Medical Center because patient records/information was anonymized and de-identified prior to analysis. All experiments were performed in accordance with the relevant guidelines and regulations.



9.RESULTS



9.1 Performance Matrics:
Figure1:shows the schematic flow of our study. We started with the original image. We preprocessed this image by decomposing it into its hemoglobin and melanin constituents. These images were then input to the U-Net to generate the segmented output. We drew contours around each cluster and used a convex hull algorithm to draw rectangles around these clusters and crop them as individual images. These cropped images were used as input to the EfficientNet, which generated a prediction along with the confidence rate.

Figure 1
Schematic flow. From left to right, the original is first decomposed into hemoglobin and melanin images. All three images are input to the U-Net which outputs a black-and-white mask image. This mask image is used to draw contours each cluster. A convex hull algorithm is applied to crop each cluster. Each cluster is input to the EfficientNet, which generates a prediction alongside the confidence rate. An open-
source implementation of the U-Net (v0.1.2) is available at: https://github.com/qubvel/segmentation_models.pytorch.
Table1shows the results of the test data for segmentation on our Dermnet dataset. The K-means clustering algorithm showed sub-optimal performance, owing to its limitations with noisy data. The SVM method showed a significant improvement in performance, that was attributed to the advantages of using SVMs to extract information from decomposition, rather than clustering algorithms. Even without the extra information, the U-Net trained without decomposition outperformed the previous two methods in terms of sensitivity. The U-Net model was also trained with decomposition and showed the highest sensitivity rate.
Table 1
Performance metrics for segmentation with dermnet images.
Method
Sensitivity
Specificity
Dice Coef
Hausdorff distance
K-means method
0.6148
0.6324
0.5165
10.487
SVM method
0.8200
0.8100
0.7123
8.138
U-Net method without decomposition
0.8953
0.7205
0.7215
8.153
U-Net method with decomposition
0.9589
0.7682
0.8126
7.165

In our results, we focused on the sensitivity metric because our objective was to assess the viability of using CAD with skin images. Although our U-Net model was not as good as the SVM model in terms of the specificity rate, it showed the best sensitivity rate, thus satisfying the objective of our study. In addition, we included the Dice coefficient and Hausdorff distance to demonstrate the performance of our methods with greater transparency. Our method showed clear improvements considering these alternative metrics. A major contributing factor7 to the underperformance of other methods is that performance of the SVM algorithm deteriorated when the images contained differences in lighting and shade. The K-means clustering method3 was also affected by the lighting and shade in the images. As our data had a significant mix of shade and lighting, the CNN was able to generalize the data better by learning to use the context of the image.
In any classification problem, it is important to set the baseline performance. We set our baseline to be the accuracy rate of the data without segmentation. The original image was input into the EfficientNet without going through the U-Net to determine the baseline accuracy rate. We compared this to the accuracy rate of the model trained to classify segmented images.
 Figure 2 shows the accuracy rates for the classification of our Dermnet dataset. We observed similar accuracy in the baseline model with and without contextual segmentation. The performance did not decrease when compared with the baseline. Thus, as we gained knowledge of the location of the disease without degrading the performance, we may say that the classification model was successfully implemented.


Figure 2
Accuracy rate for classification. The x-axis represents the Top-n accuracy metric, while the y-axis represents accuracy. The blue line is the accuracy of the model trained without segmentation. Images did not enter the U-Net before entering the EfficientNet. The gray line represents the accuracy of the model trained with segmentation. Images were segmented and cropped through the U-Net before entering the EfficientNet. The red line represents the accuracy of the model trained with segmentation and refined data. Images were segmented, cropped, and verified to ensure that segmentation had been done correctly before entering the EfficientNet. An open-
source implementation of the EfficientNet (v0.7.0) is available at https://github.com/lukemelas/EfficientNet-PyTorch.
However, we were also aware that the accuracy may have decreased due to false positives caused by areas such as the lips, which have similar characteristics to erythema. Hence, a separate model was trained with refined data, where we went through each image and excluded those that were incorrectly segmented. This improved accuracy substantially, as shown in Figure . In addition, Table 2 shows additional metrics of the area under the curve (AUC), specificity, sensitivity, and F1-score. These values are weighted averages according to the number of data contained in each class. The AUC and specificity scores are high across all methods owing to the positive correlation of these metrics with the number of classes in a classification problem. Therefore, the more meaningful metrics in this dataset are the sensitivity and F1-score. The refined segmentation method demonstrated the highest performance considering these metrics, similar to the trend shown with the accuracy metric.
Table 2
Performance metrics for classification with dermnet images.
Method
AUC
Specificity
Sensitivity
F1-score
Without segmentation
0.8207
0.9642
0.4748
0.4092
Contextual segmentation
0.8104
0.9652
0.4185
0.3876
Refined contextual segmentation
0.8802
0.9513
0.6141
0.6079

This was a result of an improved performance when there is a smaller area to search for the disease. Because we segmented only the abnormal areas of the skin, the EfficientNet model showed better performance compared to images with a larger ratio of normal skin. Thus, we can learn about the location of the disease that is present in an image and improve performance by training a CNN model to focus on particular subsections of the images. Figure 33 shows a visual representation of this claim using an implementation of the Grad-CAM method11. Activation, which is the intensity with which a model focuses on an area, is represented on a rainbow colormap. Red represents areas of highest activation, while violet represents areas of lowest activation. When trained with unsegmented data, our model focused on an area larger than that of abnormal skin. The area of activation was highest around the erythema, although there were other areas of high activation. In these cases, the model utilized the shapes of body parts for classification. This decreases performance because skin disease can appear in virtually any part of body and there is a lack of data required to form an association between the probability of a skin disease based on the body part. When trained with contextually segmented data, however, our model correctly focused only on erythema. The area of activation was highest around the erythema, while areas of low activation were demonstrated elsewhere. Not only does this add validity to our reported results, but this is also a justification for the inclusion of the segmentation phase before the classification phase because there were clear improvements in all metrics regarding the use of the U-Net before the EfficientNet.


Figure 3
Grad-CAM results for unsegmented and segmented images in our Dermnet dataset. The top row shows the original input images. The left image shows the unsegmented image and the right image shows the segmented image. The bottom row shows the result of Grad-CAM11. The left image of Grad-CAM for the unsegmented image shows that the EfficientNet model focused on a larger surface other than erythema. The right image of Grad-CAM for the segmented image shows than that the EfficientNet model correctly focused mostly on erythema.
Table 33 shows the results of the test data for segmentation on our three independent dermatoscopic image datasets—ISIC201612, ISIC201713, and HAM1000014. These datasets are some of the few publicly available datasets that had segmentation maps and classification labels. We use these datasets to verify our methods with data from independent sources. One major difference with the dermatoscopic image datasets is that they are obtained using a special dermatoscopic device. This eliminates noise in the form of background and non-skin areas, in addition to limiting the number of disease and fixing the location of skin disease within an image. This was shown to decrease the significance of our method.
Table 3
Performance metrics for segmentation with dermatoscopic datasets.
Method
Sensitivity
Specificity
Dice Coef
Hausdorff distance
ISIC2016
K-means method
0.5422
0.8249
0.5439
9.960
SVM method
0.7229
0.8602
0.6939
8.243
U-Net method without decomposition
0.9708
0.9175
0.9060
5.085
U-Net method with decomposition
0.9562
0.9422
0.9198
4.764
ISIC2017
K-means method
0.5709
0.7734
0.4926
10.567
SVM method
0.7650
0.7576
0.5967
9.388
U-Net method without decomposition
0.8971
0.8969
0.8188
5.392
U-Net method with decomposition
0.9043
0.9076
0.8199
5.338
HAM 10,000
K-means method
0.5500
0.9300
0.6381
6.807
SVM method
0.7256
0.8389
0.6674
8.381
U-Net method without decomposition
0.9542
0.9530
0.9121
4.683
U-Net method with decomposition
0.9569
0.9504
0.9166
4.621

With the ISIC2016 and ISIC2017 datasets, the performance of the less-complex K-means clustering algorithm and SVM method showed similar trends to that of our Dermnet dataset. The performance was sub-optimal, owing to the noise present in the form of varying skin and lesion colors. With the HAM10000 dataset, however, the K-means clustering algorithm outperformed the SVM method in terms of the specificity and Hausdorff distance. This performance is a result of a more statistically similar training and testing set, as they were user-defined and created after stratifying the labels. Regardless of this, the less complex methods showed sub-optimal performances with all datasets.
Across all three datasets, the U-Net models outperformed previous models in all metrics. One interesting tendency is the small performance discrepancy between the U-Net models with and without decomposition. The U-Net model without decomposition occasionally outperformed the U-Net with decomposition. This was attributed to the skin lesion being mostly fixed at the center of the image. The hemoglobin and melanin constituents aid the U-Net model to ignore areas of non-skin and to focus on areas of skin with abnormal intensities. Therefore, this did not add significant information.
Table 44 shows the results of the test data for classification on the three dermatoscopic image datasets. With the ISIC2016 dataset, the Without Segmentation method showed the highest performance in all metrics. With the ISIC2017 dataset, the Refined Contextual Segmentation method showed the highest performance by a minimal margin. With the HAM10000 dataset, the Without Segmentation method showed the highest performance in all but one category. In short, with dermatoscopic images, models trained without segmentation learned to generalize skin lesions most effectively.
Table 4
Performance metrics for classification with dermatoscopic datasets.
Method
AUC
Specificity
Sensitivity
F1-score
ISIC2016
Without segmentation
0.765
0.726
0.860
0.864
Contextual segmentation
0.719
0.641
0.826
0.833
Refined contextual segmentation
0.727
0.698
0.844
0.845
ISIC2017
Without segmentation
0.790
0.741
0.761
0.740
Contextual segmentation
0.750
0.744
0.726
0.723
Refined contextual segmentation
0.774
0.785
0.766
0.762
HAM 10,000
Without segmentation
0.891
0.933
0.866
0.871
Contextual segmentation
0.831
0.884
0.825
0.810
Refined contextual segmentation
0.871
0.919
0.873
0.866

This was a result of an improved performance when the location of the skin lesion is mostly fixed. The segmentation phase aids models to ignore areas of normal skin and to focus on areas of disease. With dermatoscopic images, this information is insignificant, as the location of the disease is static. Figure Figure44 shows a visual representation of this. The Grad-CAM images show that with both non-segmented and segmented images, the models correctly focused on the skin disease. Because of this, the segmentation phase only decreased the resolution of the image without providing useful information, thus decreasing the performance of the model.


Figure 4
Grad-CAM results for unsegmented and segmented images in the ISIC2017 dataset. For both images of Grad-CAM, the EfficientNet model correctly focused mostly on erythema.
The main contribution of our study is researching the viability of CAD in the field of dermatology. This is achieved through the increase in the classification performance of skin disease images, owing to the increase in performance of segmentation. However, our model is most effective with camera images of skin diseases with erythema, which is a limitation of our study. We chose to focus on camera images and erythema because these images are very accessible, and erythema is one of the most common symptoms of skin disease. In addition, currently we only classify diseases into 18 categories due to the limitations of the data. In the future, we plan to create a more comprehensive skin disease classification model, and this seems to be viable if enough data can be obtained. In addition, we plan to work on a method to help dermatologists with time-series analysis of patients. This seems viable with the accumulation of data through CAD

 
 
10 ADVANTAGES & DIS ADVANTAGES
Advantages:
1.It can detect the skin diseases in early state .. 
2. If users skin disease like cancer it can predict at early state and the user can take medicine for it .
3.so that the cancer will be treated at the early state.
4.Skin disease makes as great an impact as other serious medical conditions when assessed by effects on health-related quality of life.
5.This system can be used by dermatologists to give a better diagnosis and treatment to the patients. 
6.The system can be used to diagnose skin diseases at a lower cost.
7.In future, this system can be improved to detect and classify more diseases as well as their severity.
Disadvantages:
1.This may result in avoidable diagnostic inaccuracies as a result of human error, as the detection of the disease can be easily overlooked.
2.Furthermore, classification of a disease is difficult due to the strong similarities between common skin disease symptoms.
3.Allergies, irritants, genetic makeup, certain diseases, and immune system problems can cause skin conditions.
4.sometimes some detection problem can be done .











11. CONCLUSION

We have shown that even without a large dataset and high-quality images, it is possible to achieve sufficient accuracy rates. In addition, we have shown that current state-of-the-art CNN models can outperform models created by previous research, through proper data preprocessing, self-supervised learning, transfer learning, and special CNN architecture techniques. Furthermore, with accurate segmentation, we gain knowledge of the location of the disease, which is useful in the preprocessing of data used in classification, as it allows the CNN model to focus on the area of interest. Lastly, unlike previous studies, our method provides a solution to classify multiple diseases within a single image. With higher quality and a larger quantity of data, it will be viable to use state-of-the-art models to enable the use of CAD in the field of dermatology. 
The potential benefits of deep learning solutions for skin disease are tremendous and there is an unparalleled advantage in reducing the repetitive work of dermatologists and pressure on medical resources. Accurate detection is a tedious task that inevitably increases the demand for a reliable automated detection process that can be adopted routinely in the diagnostic process by expert and non-expert clinicians. Deep learning is a comprehensive subject that requires a wide range of knowledge in engineering, information, computer science, and medicine. With the continuous development of the above fields, deep learning is undergoing rapid development and has attracted the attention of numerous countries. Powered by more affordable solutions, software that can quickly collect and meaningfully process massive data, and hardware that can accomplish what people cannot, it is evident that deep learning for the identification of skin disease is a potential technique in the foreseeable future

12. FUTURE SCOPE

Early detection of lesions is a crucial step in the field of skin cancer treatment. There is a significant benefit if this can be achieved without penetrating the body. Feature extraction of skin disease is an important tool that can be used to properly analyze and explore an image [80]. Feature extraction can be simply viewed as a dimensionality reduction process; that is, converting picture data into a vector of a certain dimension with picture features. Before deep learning, this was typically determined manually by dermatologists or researchers after investigating a large number of digital skin lesion images. A well-known method for feature extraction is based on the ABCD rule of dermoscopy. ABCD stands for asymmetry, border structure, color variation, and lesion diameter. It defines the basis for disease diagnosis [81]. The extracted and fused traits such as color, texture, and Histogram of Oriented Gradient (HOG) are applied subsequently with a serial-based method. The fused features are selected afterwards by implementing a novel Boltzman entropy method [82], which can be used for the early detection. However, this typically has enormous randomness and depends on the quantity and quality of the pictures, as well as the experience of the dermatologists.
From a classification perspective, feature extraction has numerous benefits: (i) reducing classifier complexity for better generalization, (ii) improving prediction accuracy, (iii) reducing training and testing time, and (iv) enhancing the understanding and visualization of the data. The mechanism of neural networks is considerably different from that of traditional methods. Visualization indicates that the first layers are essentially calculating edge gradients and other simple operations such as SIFT [83] and HOG [84]. The folded layers combine the local patterns into a more global pattern, ultimately resulting in a more powerful feature extractor. In a study using nearly 130000 clinical dermatology images, 21 certified dermatologists tested the skin lesion classification with a single CNN, directly using pixels and image labels for end-to-end training; this had an accuracy of 0.96 for carcinoma [9]. Subsequently, researchers used deep learning to develop an automated classification system for 12 skin disorders by learning the abnormal characteristics of a malignancy and determined visual explanations from the deep network [47]. A third study combined deep learning with traditional methods such as hand-coded feature extraction and sparse coding to create a collection for melanoma detection that could yield higher performance than expert dermatologists. These results and others [85,86,87] confirm that deep learning has significant potential to reduce doctors’ repetitive work. Despite problems, it would be a significant advance if AI could reliably simulate experienced dermatologists.
13. APPENDIX
 
Source code
Intex.html
<!DOCTYPE html>
<html>
    <head>
        <title>skin diseases</title>
        <link rel="stylesheet" href="styles.css">
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<style>
    body {
        font-size: 20px;
        padding: 0%;
        margin: 0%;
    }
    #nav{
        background: #181719;
        text-decoration: none;
    }
    
     li {
        display: inline;
       
    }
    #nav ul{
                text-decoration: none;
                background-color: black;
                color: #00ffff;
                max-height: 5rem;
                min-height: 2rem;
                padding-top: 5px;
                font-size: 20px;
            }
        
    
    #nav a{
        float: right;
        color:#fff;
        text-decoration: none;
        font-size :18px;
        padding-right: 15px;
    }
    h3, header {
        color: rgb(231, 64, 64);
        font-size: 30px;
        padding-left: 20px;
        
        
    }
    h3{
        font-weight: 100%;
        margin-left: 150px;
         font-size: 50px;
    }
    header {
        height: 8px;
        margin-left: 100px;
        font-size: 50px;
    }
    
    #showcase {
        background-image: url(' https://www.girvenfp.co.nz/wp-content/uploads/2017/10/skin-disease.png');
        height: 500px;
        padding-top: 0em;
        background-position:  right;
        
    }
    h4{
        font-size:30px ;
        text-align: center;
    }
    hr {
        background-color: yellow;
        height: 1px;
        width: 20%;
    }
    #right h5{
        font-size: 30px;
        height: 1px;
        margin-left: 10px;
        
    }
    #left h5{
        font-size: 30px;
        height:1px;
        margin-left: 10px;
    }
    
</style>
    </head>
    <body>
        <div class="container">
            <nav id="nav">
            <ul> Skinnovation
            <li><a href="login.html">Prediction</a></li>
            <li><a href="regiser.html">Register</a></li>
            <li><a href="login.html">Login</a></li>
            <li><a href="index1.html">Home</a></li>
            </ul>
            </nav>
            </div>
            <section id="showcase">
            <div class="container">
                <header>Different types of </header>
                <h3>Skin Disorders</h3>
                
            </div></section>
 
            <section>
                    <h4>ABOUT PROJECT<hr></h4>
                <article id="left">
                    <h5>Problem</h5>
                    <p>Skin diseases include all conditions that irritate, clog or damage your skin, as well as skin cancer. You may inherit a skin condition or develop a skin disease. Many skin diseases cause itchiness, dry skin or rashes. Often, you can manage these symptoms with medication, proper skin care and lifestyle changes.Skin disorders vary greatly in symptoms and severity. They can be temporary or permanent, and may be painless or painful. Some have situational causes, while others may be genetic. Some skin conditions are minor, and others can be life-threatening.</p>
                </article>
                <article id="right">
                    <h5>Solution</h5>
                    <p>Skin diseases are conditions that affect your skin. These diseases may cause rashes, inflammation, itchiness or other skin changes. Some skin conditions may be genetic, while lifestyle factors may cause others. Skin disease treatment may include medications, creams or ointments, or lifestyle changes.</p>
                </article>
                <article>
                    <h4>WE CLASSIFY<hr></h4>
                    <center>
                    <img src="https://d3i71xaburhd42.cloudfront.net/975c1cb47e2bd3d54ad1d4f606dda86d0390d3fa/2-Figure1-1.png" alt="classify" width="" height="350px" id="img">
 
                </center>
            <p>A skin disease detection and classification system is a system used for detecting whether a disease is present or not, and then classifying the type of disease, if present. The classification is based on decisions taken using the features extracted through the feature extraction methods.</p></article>
            </section>
        
    </body>
</html>
 
Register.html
<!DOCTYPE html>
<html>
    <head>
        <style>
            #nav ul{
                text-decoration: none;
                background-color: black;
                color: #00ffff;
                max-height: 5rem;
                min-height: 2rem;
                padding-top: 5px;
                font-size: 20px;
            }
            li{
                float: right;
                display: inline;
 
            }
#nav a{
        color: #fff;
        text-decoration: none;
        font-size :18px;
        padding-right: 15px;
            }
.registerbtn {  
    
  background-color: #4CAF50;  
  color: white;  
  margin-top: 10px; 
  border: none;  
  cursor: pointer;  
  margin-left: 100px;
  height: 50px;
  width: 100px;  
  opacity: 0.9;  
  
}  
.registerbtn a{
    font-size: 18px;
    color: black;
    text-decoration: none;
}
.registerbtn:hover {  
  opacity: 1;  
}
body{
    margin: 0;
    padding: 0;
}
.container{
    margin-top: 5px;
    top: 50%;
    left: 50%;
    position: absolute;
    transform: translate(-50%,-50%);
}
.card{
    margin-top: 5px;
    height: 15em;
    background:rgb(255, 187, 99);
    padding: 60px 40px 50px 40px;
    border-radius: 10px;
}
#name {
    margin-top: 10px;
    border: black;
    margin-left: 50px;
    margin-right: 50px;
    margin-bottom: 10px;
    padding: 10px;
}
#acc {
    font-size: 18px;
    margin-top: 20px;
    margin-left: 50px;
}
#img{
    display: block;
  margin-left: auto;
  margin-right: auto;
  width: 50%;
}
#button {
    font-size: 18px;
    color: black;
    text-decoration: none;
}
        </style>
    </head>
    <body>
        <div>
            <nav id="nav">
            <ul>Skinnovation
            <li><a href="regiser.html">Register</a></li>
            <li><a href="login.html">Login</a></li>
            <li><a href="index1.html">Home</a></li>
            </ul>
            </nav>
            </div>
            <div class="container">
                <img src="https://img.icons8.com/plasticine/400/edit-user-male.png" height="200px" id="img">
                <div class="card">
                    <input type="text" name="fistname" placeholder="Enter Your Name" required id="name"><br>
                    <input type="email" name="email" placeholder="Enter Email ID" required id="name"><br>
                    <input type="text" name="password" placeholder="Enter Password" required id="name"><br>
                    <button type="submit" class="registerbtn" id="button"><a href="prediction.html"><b>Register</b></a></button><br> 
                    <div id="acc">
                    Already have an account? <a href="login.html">Login</a>
                </div></div>
            </div>
            </body>
</html>
Login .html
<!DOCTYPE html>
<html>
    <head>
        <style>
            body{
    margin: 0;
    padding: 0;
}
            #nav ul{
                text-decoration: none;
                background-color: black;
                color: #00ffff;
                max-height: 5rem;
                min-height: 2rem;
                padding-top: 5px;
                font-size: 20px;
            }
            li{
                float: right;
                display: inline;
 
            }
#nav a{
        color: #fff;
        text-decoration: none;
        font-size :18px;
        padding-right: 15px;
            }
.container{
    margin-top: 5px;
    top: 50%;
    left: 50%;
    position: absolute;
    transform: translate(-50%,-50%);
}           
.loginbtn {  
    
    background-color: #4CAF50;  
    color: white;  
    margin-top: 50px; 
    margin-top: 50px;
    border: none;  
    cursor: pointer;  
    margin-left: 100px;
    height: 45px;
    width: 100px;  
    opacity: 0.9;  
  }   
  .loginbtn:hover {  
  opacity: 1;  
}
#img{
    display: block;
  margin-left: auto;
  margin-right: auto;
  width: 50%;
}
.card{
    margin-top: 5px;
    height: 15em;
    background:rgb(255, 187, 99);
    padding: 60px 40px 50px 40px;
    border-radius: 10px;
    margin-bottom: 5px;
}
#name {
    margin-top: 10px;
    border: black;
    margin-left: 50px;
    margin-right: 50px;
    margin-bottom: 10px;
    padding: 10px;
    width: max-content;
}
#button a{
    font-size: 18px;
    color: black;
    text-decoration: none;
}
        </style>
    </head>
    <body>
        <div>
            <nav id="nav">
            <ul> Skinnovation
            <li><a href="regiser.html">Register</a></li>
            <li><a href="login.html">Login</a></li>
            <li><a href="index1.html">Home</a></li>
            </ul>
            </nav>
            </div>
            <div class="container">
                <img src="https://img.icons8.com/plasticine/400/edit-user-male.png" height="200px" id="img">
                <div class="card">
                    <input type="email" name="email" placeholder="Enter Email ID"  id="name" required><br>
                    <input type="text" name="password" placeholder="Enter Password"  id="name" required><br>
                    <button type="submit" class="loginbtn" id="button"><a href="prediction.html"><b>Login</b></a></button><br> 
                    </div>
                </body>
</html>
Prediction page.html
<!DOCTYPE html>
<html>
    <head>
        <style>
            body{
    margin: 0;
    padding: 0;
}
#nav{
        background: #181719;
        text-decoration: none;
    }
    
     li {
        display: inline;
       
    }
    #nav ul{
                text-decoration: none;
                background-color: black;
                color: #00ffff;
                max-height: 5rem;
                min-height: 2rem;
                padding-top: 5px;
                font-size: 20px;
            }
        
    
    #nav a{
        float: right;
        color:#fff;
        text-decoration: none;
        font-size :18px;
        padding-right: 15px;
    }
    h4{
        text-align: center;
        font-weight: 150px;
        font-size: 30px;
    }
    hr{
        background-color: yellow;
        padding: 1px;
        width: 65%;
    }
    #img{
        float: right;
        box-sizing: border-box;
        width: 25%;
        padding: 30px;
        margin-right: 10em;
        margin-top: 0%;
    }
    #pr{
        margin-top: 50px;
        margin-left: 50px;
        float: left;
        padding: 0 30px;
        width: 50%;
        box-sizing: border-box;
        box-shadow: #181719;
    }
        </style>
    </head>
    <body>
 
        <div>
            <nav id="nav">
            <ul> Skin Diseases Detection
            <li><a href="logout.html">Logout</a></li>
            <li><a href="index1.html">Home</a></li>
            </ul>
            </nav>
            <h4 class="h">
                Skinnovation-AI-Based localization and classification of skin diseases with erythema<hr>
            </h4>
            <article id="pr">
            <p>Nowadays people are sufferring from skin diseases,More than 125 million people suffering from Psoriasis also skin cancer rate is rapidly increasing over the last few decades especially Melanoma is most diversifying skin cancer.If skin diseases are not treated at an earlier state,then it may lead to complications in the body including spreading of the infection from one individual to the other.This skin disease can be prevented by investicating the infected region atb an early stage.The characterstic of the skin images is diversified so that it is a challenging job to device an efficient and robust algoritham for automatic detection of skin diseases and its scverity.Skin tone and skin colour play an important role in skin diseases detection.Color and coarseness of skin are visually different.Automatic processing of such images for skin analysis requires quantitative discriminator to differentiate the disease.</p>
    </article>
    <img src="C:\Users\ELCOT\Desktop\skin project\images/skin.jpg" max-height="100px" id="img">
        </body>
</html>
 
Logout.html
<!DOCTYPE html>
<html>
    <head>
        <style>
            body{
    margin: 0;
    padding: 0;
    
}
#nav{
        background: #181719;
        text-decoration: none;
        
    }
    
     li {
        display: inline;
       
    }
    #nav ul{
                text-decoration: none;
                background-color: black;
                color: #00ffff;
                max-height: 5rem;
                min-height: 2rem;
                padding-top: 5px;
                font-size: 20px;
            }
        
    
    #nav a{
        float: right;
        color:#fff;
        text-decoration: none;
        font-size :18px;
        padding-right: 15px;
    }
    .logoutbtn {  
    
    background-color: #4CAF50;  
    color: black;  
    cursor: pointer;  
    height: 45px;
    width: 100px;  
    opacity: 0.9;  
  }   
  #button a{
    text-decoration: none;
    color: black;
  }
  .loginbtn:hover {  
  opacity: 1;  
}
.container{
    display: block;
    text-align: center;
    letter-spacing: 1px;
}
p{
    font-size: 20px;
    color: #4CAF50;
    
}
h3{
    font-size: 25px;
}
#img{
    padding: 5px;
    background-color: white;
}
        </style>
    </head>
    <body>
<div>
            <nav id="nav">
            <ul> Skinnovation
            <li><a href="regiser.html">Register</a></li>
            <li><a href="login.html">Login</a></li>
            <li><a href="index1.html">Home</a></li>
            </ul>
            </nav>
            </div>
            <div class="container">
                <img src="C:\Users\ELCOT\Desktop\skin project\images/logout.png" id="img">
                <h3>Successfully Logged Out!</h3>
                <p>Login for more information</p>
                <button type="submit" class="logoutbtn" id="button"><a href="login.html"><b>Login</b></a></button>
            </div>
    </body>
</html>

 
GitHub Link

https://github.com/IBM-EPBL/IBM-Project-49194-1660816624

Demo link

https://drive.google.com/file/d/1MRErqn90ilzY3jnYE-9UsQqExQK-F6QH/view?usp=share_link
